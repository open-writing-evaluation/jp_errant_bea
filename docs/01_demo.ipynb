{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demos\n",
    "\n",
    "In this notebook, we will do some demonstrations on how to use jp-errant package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation instruction: \n",
    "\n",
    "First, create a conda enviroment with Python 3.12 using the following command line \n",
    "- `conda create --name jp_errant_test python==3.12`\n",
    "- `conda activate jp_errant_test`\n",
    "\n",
    "Clone the `jp-errant` repo if you haven't done so:\n",
    "- `git clone https://github.com/open-writing-evaluation/jp-errant.git`\n",
    "- `cd jp-errant`  # move to the jp-errant folder of the cloned repo\n",
    "\n",
    "Check out to the latest branch (`minh`, or `dev`, or `main`):\n",
    "- `git checkout minh`\n",
    "\n",
    "Install dependencies:\n",
    "- `pip install -r requirements.txt` \n",
    "\n",
    "Then, install the package using this line:\n",
    "- `pip install -e .`  # install the jp-errant package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jp_errant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_errant.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:59:00 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 8.99MB/s]                    \n",
      "2025-02-24 19:59:00 INFO: Downloaded file to C:\\Users\\Minh UBC\\stanza_resources\\resources.json\n",
      "2025-02-24 19:59:01 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-02-24 19:59:01 WARNING: GPU requested, but is not available!\n",
      "2025-02-24 19:59:01 INFO: Using device: cpu\n",
      "2025-02-24 19:59:01 INFO: Loading: tokenize\n",
      "2025-02-24 19:59:01 INFO: Loading: mwt\n",
      "2025-02-24 19:59:01 INFO: Loading: pos\n",
      "2025-02-24 19:59:03 INFO: Loading: lemma\n",
      "2025-02-24 19:59:03 INFO: Loading: depparse\n",
      "2025-02-24 19:59:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# initialize jp-errant using default Stanza nlp\n",
    "annotator = jp_errant.load(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jp_errant.annotator.Annotator"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"Today is good day.\"\n",
    "corrected = \"Today is a good day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do annotations\n",
    "original_tokens = annotator.parse(original)\n",
    "corrected_tokens = annotator.parse(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate \n",
    "edits = annotator.annotate(orig=original_tokens, \n",
    "                           cor=corrected_tokens\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig: [2, 2, ''], Cor: [2, 3, 'a'], Type: 'M:DET'\n"
     ]
    }
   ],
   "source": [
    "for edit_item in edits:\n",
    "    print(edit_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:59:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 6.56MB/s]                    \n",
      "2025-02-24 19:59:05 INFO: Downloaded file to C:\\Users\\Minh UBC\\stanza_resources\\resources.json\n",
      "2025-02-24 19:59:05 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2025-02-24 19:59:05 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | gsdsimp          |\n",
      "| pos       | gsdsimp_charlm   |\n",
      "| lemma     | gsdsimp_nocharlm |\n",
      "| depparse  | gsdsimp_charlm   |\n",
      "================================\n",
      "\n",
      "2025-02-24 19:59:05 WARNING: GPU requested, but is not available!\n",
      "2025-02-24 19:59:05 INFO: Using device: cpu\n",
      "2025-02-24 19:59:05 INFO: Loading: tokenize\n",
      "2025-02-24 19:59:06 INFO: Loading: pos\n",
      "2025-02-24 19:59:08 INFO: Loading: lemma\n",
      "2025-02-24 19:59:08 INFO: Loading: depparse\n",
      "2025-02-24 19:59:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# test for Chinese language, default Stanza\n",
    "annotator_zh = jp_errant.load(lang=\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text_zh = \"冬阴功是泰国最著名的菜之一，它虽然不是很豪华，但它的味确实让人上瘾，做法也不难、不复杂。\"\n",
    "corrected_text_zh = \"冬阴功是泰国最著名的菜之一，虽然它不是很豪华，但它的味确实让人上瘾，做法也不难、不复杂。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_zh_tokens = annotator_zh.parse(original_text_zh)\n",
    "corrected_zh_tokens = annotator_zh.parse(corrected_text_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing: 它虽然 虽然它 0.75 0.7200819306244606\n"
     ]
    }
   ],
   "source": [
    "edits = annotator_zh.annotate(orig = original_zh_tokens,\n",
    "                              cor=corrected_zh_tokens\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig: [11, 13, '它 虽然'], Cor: [11, 13, '虽然 它'], Type: 'R:WO'\n"
     ]
    }
   ],
   "source": [
    "for edit in edits:\n",
    "    print(edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:59:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 14.6MB/s]                    \n",
      "2025-02-24 19:59:10 INFO: Downloaded file to C:\\Users\\Minh UBC\\stanza_resources\\resources.json\n",
      "2025-02-24 19:59:10 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2025-02-24 19:59:11 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../trained...kenizer.pt |\n",
      "| pos       | gsdsimp_charlm          |\n",
      "| lemma     | gsdsimp_nocharlm        |\n",
      "| depparse  | gsdsimp_charlm          |\n",
      "=======================================\n",
      "\n",
      "2025-02-24 19:59:11 INFO: Using device: cpu\n",
      "2025-02-24 19:59:11 INFO: Loading: tokenize\n",
      "2025-02-24 19:59:11 INFO: Loading: pos\n",
      "2025-02-24 19:59:13 INFO: Loading: lemma\n",
      "2025-02-24 19:59:14 INFO: Loading: depparse\n",
      "2025-02-24 19:59:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# test for Chinese language, custom trained Stanza following LTP style tokenizer\n",
    "SAVED_MODEL_FOLDER = \"../trained_models/\"  # point to the folder with save pt files\n",
    "tokenize_path = SAVED_MODEL_FOLDER + \"UD_Chinese-GSDSimpLTP_model/saved_models/tokenize/zh_gsdsimpltp_tokenizer.pt\"\n",
    "nlp = stanza.Pipeline(lang=\"zh\", \n",
    "                      processors=\"tokenize,pos,lemma,depparse\",\n",
    "                      tokenize_model_path=tokenize_path,  \n",
    "                      )\n",
    "\n",
    "# load jp-errant with custom Stanza nlp component\n",
    "annotator_zh = jp_errant.load(lang=\"zh\", nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_zh_tokens = annotator_zh.parse(original_text_zh)\n",
    "corrected_zh_tokens = annotator_zh.parse(corrected_text_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "冬阴功  是  泰国  最  著名  的  菜  之  一  ，  它  虽然  不是  很  豪华  ，  但  它  的  味  确实  让  人  上瘾  ，  做法  也  不难  、  不  复杂  。  \n",
      " ----------\n",
      "Corrected text\n",
      "冬阴功  是  泰国  最  著名  的  菜  之  一  ，  虽然  它  不是  很  豪华  ，  但  它  的  味  确实  让  人  上瘾  ，  做法  也  不难  、  不  复杂  。  "
     ]
    }
   ],
   "source": [
    "# print tokenized words\n",
    "print(\"Original text\")\n",
    "for item in original_zh_tokens.iter_words():\n",
    "    print(item.text, end=\"  \")\n",
    "\n",
    "print(\"\\n\", \"-\"*10)\n",
    "print(\"Corrected text\")\n",
    "for item in corrected_zh_tokens.iter_words():\n",
    "    print(item.text, end=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing: 它虽然 虽然它 0.75 0.7200819306244606\n",
      "Orig: [10, 12, '它 虽然'], Cor: [10, 12, '虽然 它'], Type: 'R:WO'\n"
     ]
    }
   ],
   "source": [
    "edits = annotator_zh.annotate(orig = original_zh_tokens,\n",
    "                              cor=corrected_zh_tokens\n",
    "                              )\n",
    "for edit in edits:\n",
    "    print(edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test multi-reference command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"./data/multi_references/en_input.txt\"         # original sentence\n",
    "REFERENCE_FILE = \"./data/multi_references/en_ref_multi.tsv\" # corrected sentences, separated by tab \\t\n",
    "OUTPUT_FILE = \"./data/multi_references/en_output.m2\"        # m2 output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jp_errant_parallel [-h] [options] -orig ORIG -cor COR [COR ...] [-tsv yes] -out OUT\n",
      "\n",
      "Align parallel text files and extract and classify the edits.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -orig ORIG            The path to the original text file.\n",
      "  -cor COR [COR ...]    The paths to >= 1 corrected text files.\n",
      "  -out OUT              The output filepath.\n",
      "  -lang LANG            The 2-letter language code (default: en).\n",
      "  -lev                  Align using standard Levenshtein (default: True).\n",
      "  -merge {rules,all-split,all-merge,all-equal}\n",
      "                        Choose a merging strategy for automatic alignment.\n",
      "                        rules: Use a rule-based merging strategy (default)\n",
      "                        all-split: Merge nothing: MSSDI -> M, S, S, D, I\n",
      "                        all-merge: Merge adjacent non-matches: MSSDI -> M, SSDI\n",
      "                        all-equal: Merge adjacent same-type non-matches: MSSDI -> M, SS, D, I\n"
     ]
    }
   ],
   "source": [
    "!jp_errant_parallel --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resources...\n",
      "Processing parallel files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:59:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|          | 0.00/52.5k [00:00<?, ?B/s]\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 12.1MB/s]                    \n",
      "2025-02-24 19:59:24 INFO: Downloaded file to C:\\Users\\Minh UBC\\stanza_resources\\resources.json\n",
      "2025-02-24 19:59:25 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-02-24 19:59:25 WARNING: GPU requested, but is not available!\n",
      "2025-02-24 19:59:25 INFO: Using device: cpu\n",
      "2025-02-24 19:59:25 INFO: Loading: tokenize\n",
      "2025-02-24 19:59:26 INFO: Loading: mwt\n",
      "2025-02-24 19:59:26 INFO: Loading: pos\n",
      "2025-02-24 19:59:28 INFO: Loading: lemma\n",
      "2025-02-24 19:59:29 INFO: Loading: depparse\n",
      "2025-02-24 19:59:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "!jp_errant_parallel -orig {INPUT_FILE} -cor {REFERENCE_FILE} -out {OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_ZH = \"./data/multi_references/zh_input.txt\"         # original sentence\n",
    "REFERENCE_FILE_ZH = \"./data/multi_references/zh_ref.tsv\"       # corrected sentences, separated by tab \\t\n",
    "OUTPUT_FILE_ZH = \"./data/multi_references/zh_output.m2\"        # m2 output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resources...\n",
      "Processing parallel files...\n",
      "Comparing: 它虽然 虽然它 0.75 0.7200819306244606\n",
      "Comparing: 大 :大 0.8 0\n",
      "Comparing: 搾好 榨好 1.0 0.5604585667221499\n",
      "Comparing: 叶三 叶三 1.0 0.9999999949197186\n",
      "Comparing: 辣椒6 辣椒六 0.75 0.777275938334677\n",
      "Comparing: 4 四 0.0 0.5606951171827526\n",
      "Comparing: 草菇10 草菇十 0.6666666666666666 0\n",
      "Comparing: 死爱 爱死 0.5 0.8413336325438497\n",
      "Comparing: 对外国人 外国人对 0.75 0.5867633548646929\n",
      "Comparing: 是 在 0.3333333333333333 0.7543451641312413\n",
      "Comparing: 的心里 都 0.2 0\n",
      "Comparing: 在 使 0.3333333333333333 0.8399053672795396\n",
      "Comparing: 猛近 猛进 1.0 0.9481206153284796\n",
      "Comparing: 日益严重造成了 造成了日益严重 0.5454545454545454 0.6659675649793019\n",
      "Comparing: 日益严重造成了 造成了日益严重 0.5454545454545454 0.6659675649793019\n",
      "Comparing: 空气 的 0.0 0\n",
      "Comparing: 有助 无助 0.7272727272727273 0.9148432177750987\n",
      "Comparing: 人生 人 0.5454545454545454 0\n",
      "Comparing: 建康 健康 1.0 0.9501210549857415\n",
      "Comparing: 人生 人体 0.46153846153846156 0.9037275247914941\n",
      "Comparing: 题 题— 0.8 0\n",
      "Comparing: 。 ， 0.0 0.7181735669053702\n",
      "Comparing: 人生 人体 0.46153846153846156 0.9037275247914941\n",
      "Comparing: 题 题— 0.8 0\n",
      "Comparing: 。 ， 0.0 0.7181735669053702\n",
      "Comparing: 以及 其 0.3333333333333333 0\n",
      "Comparing: 要 次要 0.75 0\n",
      "Comparing: 周知越 周知越 1.0 1.0\n",
      "Comparing: 建立 发展 0.3333333333333333 0.7104887241354746\n",
      "Comparing: 越对 ， 0.0 0\n",
      "Comparing: 有利 有利 1.0 1.0\n",
      "Comparing: 周知越 周知越 1.0 1.0\n",
      "Comparing: 建立 发展 0.3333333333333333 0.7104887241354746\n",
      "Comparing: 越对 ， 0.0 0\n",
      "Comparing: 对我 在我 0.6 0.9337960720256627\n",
      "Comparing: 毒 有毒 0.5714285714285714 0\n",
      "Comparing: 毒 有毒 0.5714285714285714 0\n",
      "Comparing: 每天 每天 1.0 0.9999999992784285\n",
      "Comparing: 毒 有毒 0.5714285714285714 0\n",
      "Comparing: 。 ， 0.0 0.7181735669053702\n",
      "Comparing: 在 得离 0.2857142857142857 0\n",
      "Comparing: 每天 每天 1.0 0.9999999992784285\n",
      "Comparing: 毒 有毒 0.5714285714285714 0\n",
      "Comparing: 。 ， 0.0 0.7181735669053702\n",
      "Comparing: 近 附近 0.75 0\n",
      "Comparing: 不官 不管 1.0 0.9237996367208232\n",
      "Comparing: 、 还是 0.0 0\n",
      "Comparing: 进 进去 0.75 0\n",
      "Comparing: 去 ， 0.0 0.34961245615602965\n",
      "Comparing: 这样 都 0.0 0\n",
      "Comparing: 建康 健康 1.0 0.9501210549857415\n",
      "Comparing: 沙尘暴 沙尘暴 1.0 1.0\n",
      "Comparing: 关主 关注 1.0 0.9431044256606669\n",
      "Comparing: 最难 最难 1.0 1.0\n",
      "Comparing: 生名 生命 1.0 0.9037453282806582\n",
      "Comparing: 很 有很大 0.5454545454545454 0\n",
      "Comparing: 生名 生命 1.0 0.9037453282806582\n",
      "Comparing: 很 很有 0.6666666666666666 0\n",
      "Comparing: 有很大 很有 0.42857142857142855 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:59:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|          | 0.00/52.5k [00:00<?, ?B/s]\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 6.61MB/s]                    \n",
      "2025-02-24 19:59:37 INFO: Downloaded file to C:\\Users\\Minh UBC\\stanza_resources\\resources.json\n",
      "2025-02-24 19:59:37 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2025-02-24 19:59:38 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | gsdsimp          |\n",
      "| pos       | gsdsimp_charlm   |\n",
      "| lemma     | gsdsimp_nocharlm |\n",
      "| depparse  | gsdsimp_charlm   |\n",
      "================================\n",
      "\n",
      "2025-02-24 19:59:38 WARNING: GPU requested, but is not available!\n",
      "2025-02-24 19:59:38 INFO: Using device: cpu\n",
      "2025-02-24 19:59:38 INFO: Loading: tokenize\n",
      "2025-02-24 19:59:39 INFO: Loading: pos\n",
      "2025-02-24 19:59:42 INFO: Loading: lemma\n",
      "2025-02-24 19:59:42 INFO: Loading: depparse\n",
      "2025-02-24 19:59:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# testing parallel to m2 command line with Chinese (zh) language\n",
    "# please NOTE: it does **NOT** support loading LTP files yet. The command line still uses default stanza nlp processor\n",
    "!jp_errant_parallel -orig {INPUT_FILE_ZH} -cor {REFERENCE_FILE_ZH} -out {OUTPUT_FILE_ZH} -lang zh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
